
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Graph Neural Networks &#8212; Deep Learning for Molecules and Materials</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/a11y.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/custom.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. Attention Layers" href="attention.html" />
    <link rel="prev" title="2. Standard Layers" href="layers.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Deep Learning for Molecules and Materials</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Deep Learning for Molecules and Materials
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  A. Math Review
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../math/tensors-and-shapes.html">
   1. Tensors and Shapes
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  B. Machine Learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/introduction.html">
   1. Introduction to Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/regression.html">
   2. Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml/classification.html">
   3. Classification
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  C. Deep Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   1. Introduction to Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="layers.html">
   2. Standard Layers
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Graph Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="attention.html">
   4. Attention Layers
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Made with <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/dl/gnn.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/whitead/dmol-book/master?urlpath=tree/dl/gnn.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/whitead/dmol-book/blob/master/dl/gnn.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representing-a-graph">
   3.1. Representing a Graph
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-this-notebook">
   3.2. Running This Notebook
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-graph-neural-network">
   3.3. A Graph Neural Network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-simple-gnn">
     3.3.1. A simple GNN
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kipf-welling-gcn">
   3.4. Kipf &amp; Welling GCN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gcn-implementation">
     3.4.1. GCN Implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solubility-example">
   3.5. Solubility Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#message-passing-viewpoint">
   3.6. Message Passing Viewpoint
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gated-graph-neural-network">
   3.7. Gated Graph Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#battaglia-general-equations">
   3.8. Battaglia General Equations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reformulating-gcn-into-battaglia-equations">
     3.8.1. Reformulating GCN into Battaglia equations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cited-references">
   3.9. Cited References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="graph-neural-networks">
<h1><span class="section-number">3. </span>Graph Neural Networks<a class="headerlink" href="#graph-neural-networks" title="Permalink to this headline">¶</a></h1>
<p>The biggest difficulty for deep learning with molecules is the choice and computation of “descriptors”. Graph neural networks (GNNs) are a category of deep neural networks whose inputs are graphs. As usual, they are composed of specific layers that input a graph and those layers are what we’re interested in. You can find reviews of GNNs in X, Y, and Z. Before we dive too deep into them, we must first understand how a graph is represented and how molecules are converted into graphs.</p>
<div class="section" id="representing-a-graph">
<h2><span class="section-number">3.1. </span>Representing a Graph<a class="headerlink" href="#representing-a-graph" title="Permalink to this headline">¶</a></h2>
<p>A graph <span class="math notranslate nohighlight">\(\mathbf{G}\)</span> is a set of nodes <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> and edges <span class="math notranslate nohighlight">\(\mathbf{E}\)</span>. In our setting, node <span class="math notranslate nohighlight">\(i\)</span> is defined by a vector <span class="math notranslate nohighlight">\(\vec{v}_i\)</span>, so that the set of nodes can be written as a rank 2 tensor. The edges can be represented as an adjacency matrix <span class="math notranslate nohighlight">\(\mathbf{E}\)</span>, where if <span class="math notranslate nohighlight">\(e_{ij} = 1\)</span> then nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are connected by an edge. In many fields, graphs are often immediately simplified to be directed and acyclic, which simplifies things. Molecules are instead undirected and have cycles (rings). Thus, our adjacency matrices are always symmetric <span class="math notranslate nohighlight">\(e_{ij} = e_{ji}\)</span>. Often our edges themselves have features, so that <span class="math notranslate nohighlight">\(e_{ij}\)</span> is itself a vector. Then the adjacency matrix becomes a rank 3 tensor. Examples of edge features might be covalent bond order or distance between two nodes.</p>
<div class="figure align-default" id="methanol">
<a class="reference internal image-reference" href="../_images/methanol.jpg"><img alt="../_images/methanol.jpg" src="../_images/methanol.jpg" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2 </span><span class="caption-text">Methanol with atoms numbered so that we can convert it to a graph.</span><a class="headerlink" href="#methanol" title="Permalink to this image">¶</a></p>
</div>
<p>Let’s see how a graph can be constructed from a molecule. Consider methanol, shown in Figure <a class="reference internal" href="#methanol"><span class="std std-numref">Fig. 3.2</span></a>. I’ve numbered the atoms so that we have an order for defining the nodes/edges. First, the node features. You can use anything for node features, but often we’ll begin with one-hot encoded feature vectors:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Node</p></th>
<th class="head"><p>C</p></th>
<th class="head"><p>H</p></th>
<th class="text-align:right head"><p>O</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>2</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>3</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>4</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>5</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>6</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
</tbody>
</table>
<p><span class="math notranslate nohighlight">\(\mathbf{V}\)</span> will be the combined feature vectors of these nodes. The adjacency matrix <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> will look like:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
<th class="head"><p>4</p></th>
<th class="head"><p>5</p></th>
<th class="text-align:right head"><p>6</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>2</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>3</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>4</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>5</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>6</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
</tbody>
</table>
<p>Take a moment to understand these two. For example, notice that rows 1, 2, and 3 only have the 4th column as non-zero. That’s because atoms 1-3 are bonded only to carbon (atom 4). Also, the diagonal is always 0 because atoms cannot be bonded with themselves.</p>
<p>We’ll now begin with a function which can convert a smiles string into this representation.</p>
</div>
<div class="section" id="running-this-notebook">
<h2><span class="section-number">3.2. </span>Running This Notebook<a class="headerlink" href="#running-this-notebook" title="Permalink to this headline">¶</a></h2>
<p>Click the  <i aria-label="Launch interactive content" class="fas fa-rocket"></i>  above to launch this page as an interactive Google Colab. See details below on installing packages, either on your own environment or on Google Colab</p>
<div class="dropdown admonition tip">
<p class="admonition-title">Tip</p>
<p>To install packages, execute this code in a new cell</p>
<p><strong>For Google Colab</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!wget -c https://repo.continuum.io/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh
!chmod +x Miniconda3-py37_4.8.3-Linux-x86_64.sh
!time bash ./Miniconda3-py37_4.8.3-Linux-x86_64.sh -b -f -p /usr/local
!time conda install -q -y -c conda-forge rdkit

import sys
sys.path.append(&#39;/usr/local/lib/python3.7/site-packages/&#39;)

!conda install -c conda-forge graphviz
!pip install jupyter-book matplotlib numpy tensorflow pydot seaborn Pillow
</pre></div>
</div>
<p><strong>For Conda Env</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!conda install -c conda-forge graphviz
!pip install jupyter-book matplotlib numpy tensorflow pydot seaborn Pillow
</pre></div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">rdkit</span><span class="o">,</span> <span class="nn">rdkit.Chem</span><span class="o">,</span> <span class="nn">rdkit.Chem.rdDepictor</span><span class="o">,</span> <span class="nn">rdkit.Chem.Draw</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;notebook&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;dark&#39;</span><span class="p">,</span>  <span class="p">{</span><span class="s1">&#39;xtick.bottom&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;ytick.left&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;xtick.color&#39;</span><span class="p">:</span> <span class="s1">&#39;#666666&#39;</span><span class="p">,</span> <span class="s1">&#39;ytick.color&#39;</span><span class="p">:</span> <span class="s1">&#39;#666666&#39;</span><span class="p">,</span>
                        <span class="s1">&#39;axes.edgecolor&#39;</span><span class="p">:</span> <span class="s1">&#39;#666666&#39;</span><span class="p">,</span> <span class="s1">&#39;axes.linewidth&#39;</span><span class="p">:</span>     <span class="mf">0.8</span> <span class="p">,</span> <span class="s1">&#39;figure.dpi&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">})</span>
<span class="n">color_cycle</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;#1BBC9B&#39;</span><span class="p">,</span> <span class="s1">&#39;#F06060&#39;</span><span class="p">,</span> <span class="s1">&#39;#5C4B51&#39;</span><span class="p">,</span> <span class="s1">&#39;#F3B562&#39;</span><span class="p">,</span> <span class="s1">&#39;#6e5687&#39;</span><span class="p">]</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.prop_cycle&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">cycler</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">color_cycle</span><span class="p">)</span> 
<span class="n">soldata</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://dataverse.harvard.edu/api/access/datafile/3407241?format=original&amp;gbrecs=true&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">my_elements</span> <span class="o">=</span> <span class="p">{</span><span class="mi">6</span><span class="p">:</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="mi">8</span><span class="p">:</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;H&#39;</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>The hidden cell below defines our function <code class="docutils literal notranslate"><span class="pre">smiles2graph</span></code>. This creates one-hot node feature vectors for the element C, H, and O. It also creates an adjacency tensor with one-hot bond order being the feature vector.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">smiles2graph</span><span class="p">(</span><span class="n">sml</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Argument for the RD2NX function should be a valid SMILES sequence</span>
<span class="sd">    returns: the graph</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">sml</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">AddHs</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">order_string</span> <span class="o">=</span> <span class="p">{</span><span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">SINGLE</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">DOUBLE</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
                    <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">TRIPLE</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
                    <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">AROMATIC</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">()))</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">my_elements</span><span class="p">)))</span>
    <span class="n">lookup</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">my_elements</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">():</span>
        <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">GetIdx</span><span class="p">(),</span> <span class="n">lookup</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">GetAtomicNum</span><span class="p">())]</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="n">adj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetBonds</span><span class="p">():</span>
        <span class="n">u</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span><span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>
        <span class="n">v</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span><span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>        
        <span class="n">order</span> <span class="o">=</span> <span class="n">j</span><span class="o">.</span><span class="n">GetBondType</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_string</span><span class="p">:</span>
            <span class="n">order</span> <span class="o">=</span> <span class="n">order_string</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Warning</span><span class="p">(</span><span class="s1">&#39;Ignoring bond order&#39;</span> <span class="o">+</span> <span class="n">order</span><span class="p">)</span>
        <span class="n">adj</span><span class="p">[</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">order</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>        
        <span class="n">adj</span><span class="p">[</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">order</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>        
    <span class="k">return</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">smiles2graph</span><span class="p">(</span><span class="s1">&#39;CO&#39;</span><span class="p">)</span>
<span class="n">nodes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 0., 1.],
       [0., 0., 1.],
       [0., 0., 1.]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="a-graph-neural-network">
<h2><span class="section-number">3.3. </span>A Graph Neural Network<a class="headerlink" href="#a-graph-neural-network" title="Permalink to this headline">¶</a></h2>
<p>A graph neural network (GNN) is a neural network with two defining attributes:</p>
<ol class="simple">
<li><p>It’s input is a graph</p></li>
<li><p>It’s output is permutation invariant</p></li>
</ol>
<p>We can understand clearly the first point. Here, a graph permutation means re-ordering our nodes. In our methanol example above, we could have easily made the carbon be atom 1 instead of atom 4. Our new adjacency matrix would then be:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
<th class="head"><p>4</p></th>
<th class="head"><p>5</p></th>
<th class="text-align:right head"><p>6</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>2</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>3</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>4</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>5</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>6</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
</tbody>
</table>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>Ok, so technically we might want our GNN to be permutation <em>equivariant</em>. If our GNN outputs per-node features, then obviously if we swap the node order of input, we want our per-node output to swap.</p>
</div>
<p>A GNN is permutation invariant if the output is insensitive to these kind of exchanges. Of course, there may exist GNNs out there which are not permutation invariant, especially if they are for trees where it is possible to deterministically order all nodes. Yet all the GNNs used in chemistry and most of the deep learning work is concerned with GNNs that are permutation invariant.</p>
<div class="section" id="a-simple-gnn">
<h3><span class="section-number">3.3.1. </span>A simple GNN<a class="headerlink" href="#a-simple-gnn" title="Permalink to this headline">¶</a></h3>
<p>We will often mention a GNN when we really mean a layer from a GNN. Most GNNs implement a specific layer that can deal with graphs, and so usually we are only concerned with this layer. Let’s see an example of a simple layer for a GNN:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e03aabff-11fd-4ed7-9df4-797b21d7123a">
<span class="eqno">(3.9)<a class="headerlink" href="#equation-e03aabff-11fd-4ed7-9df4-797b21d7123a" title="Permalink to this equation">¶</a></span>\[\begin{equation}
f_k = \sigma\left( \sum_i \sum_jn_{ij}w_{jk}  \right)
\end{equation}\]</div>
<p>This equation shows that we first multiply every node feature by trainable weights <span class="math notranslate nohighlight">\(w_{jk}\)</span>, sum over all node features, and then apply an activation. This will yield a single feature vector for the graph. Is this equation permutation invariant? Yes, because the node index in our expression is index <span class="math notranslate nohighlight">\(i\)</span> which can be re-ordered without affecting the output.</p>
<p>Let’s see an example that is similar, but not permutation invariant:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a1f16c60-2ca7-4f27-b72b-8b23b42dda55">
<span class="eqno">(3.10)<a class="headerlink" href="#equation-a1f16c60-2ca7-4f27-b72b-8b23b42dda55" title="Permalink to this equation">¶</a></span>\[\begin{equation}
f_k = \sigma\left( \sum_i n_{ij}w_{ik}  \right)
\end{equation}\]</div>
<p>This is a small change. We have one weight vector per node now. This makes the trainable weights depend on the ordering of the nodes. Then if we swap the node ordering, our weights will no longer align. So if we were to input two methanol molecules, which should have the same output, but we switched two atom numbers, we would get different answers. These simple examples differ from real GNNs in two important ways: (i) they give a single feature vector output, which throws away per-node information, and (ii) they do not use the adjacency matrix. Let’s see a real GNN that has these properties while maintaining permutation invariance.</p>
</div>
</div>
<div class="section" id="kipf-welling-gcn">
<h2><span class="section-number">3.4. </span>Kipf &amp; Welling GCN<a class="headerlink" href="#kipf-welling-gcn" title="Permalink to this headline">¶</a></h2>
<p>One of the first popular GNNs is the Kipf &amp; Welling graph convolutional network (GCN) <a class="bibtex reference internal" href="layers.html#kipf2016semi" id="id1">[KW16]</a>. Although some people consider GCNs to be a broad class of GNNs, we’ll use GCNs to refer specifically the Kipf &amp; Welling GCN.
Thomas Kipf has written an <a class="reference external" href="https://tkipf.github.io/graph-convolutional-networks/">excellent article introducing the GCN</a>. I will not repeat this article, so please take a look at it.</p>
<p>The input to a GCN layer is <span class="math notranslate nohighlight">\(\mathbf{V}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> and it outputs an updated <span class="math notranslate nohighlight">\(\mathbf{V}'\)</span>. Each node feature vector is updated. The way it updates a node feature vector is by averaging the feature vectors of its neigbhors, as determined by <span class="math notranslate nohighlight">\(\mathbf{E}\)</span>. The choice of averaging over neigbhors is what makes a GCN layer permutation invariant. Averaging over neighbors is not trainable, so we must add trainable parameters. We multiply the neighbor features by a trainable matrix before the averaging, which gives the GCN the ability to learn. In Einstein notation, this process is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7787ff34-3fe6-452c-8f51-e8f5cf482c50">
<span class="eqno">(3.11)<a class="headerlink" href="#equation-7787ff34-3fe6-452c-8f51-e8f5cf482c50" title="Permalink to this equation">¶</a></span>\[\begin{equation}
v_{il} = \sigma\left(\frac{1}{d_i}e_{ij}v_{jk}w_{lk}\right)
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(i\)</span> is the node we’re considering, <span class="math notranslate nohighlight">\(j\)</span> is the neighbor index, <span class="math notranslate nohighlight">\(k\)</span> is the node input feature, <span class="math notranslate nohighlight">\(l\)</span> is the ouput node feature, <span class="math notranslate nohighlight">\(d_i\)</span> is the degree of node i (which makes it an average instead of sum), <span class="math notranslate nohighlight">\(e_{ij}\)</span> isolates neighbors so that all non-neighbor <span class="math notranslate nohighlight">\(v_{jk}\)</span>s are zero, <span class="math notranslate nohighlight">\(\sigma\)</span> is our activation, and <span class="math notranslate nohighlight">\(w_{lk}\)</span> is the trainable weights. This equation is a mouthful, but it truly just is the average over neighbors with a trainable matrix thrown in. One common modification is to make all nodes neighbors of themselves. This is so that the output node features <span class="math notranslate nohighlight">\(v_{il}\)</span> depends on the input features <span class="math notranslate nohighlight">\(v_{ik}\)</span>. We do not need to change our equation, just make the adjacency matrix have <span class="math notranslate nohighlight">\(1\)</span>s on the diagonal instead of <span class="math notranslate nohighlight">\(0\)</span> by adding the identity matrix during pre-processing.</p>
<p>Building understanding about the GCN is important for understanding other GNNs. You can view the GCN layer as a way to “communicate” between a node and its neigbhors. The output for node <span class="math notranslate nohighlight">\(i\)</span> will depend only on its immediate neigbhors. For chemistry, this is not satisfactory. So you can stack multiple layers. If you have two layers, then the output for node <span class="math notranslate nohighlight">\(i\)</span> will include information about node <span class="math notranslate nohighlight">\(i\)</span>’s neighbors’ neigbhors. Another important detail to understand in GCNs is that the averaging procedure accomplishes two goals: (i) it gives permutation invariance by removing the effect of neighbor order and (ii) it prevents a change in magnitude in node features. A sum would accomplish (i) but would cause the magnitude of the node features to grow after each layer. Of course, you could ad-hoc put a batch normalization layer after each GCN layer to keep output magnitudes stable but averaging is easy.</p>
<div class="figure align-default" id="dframe">
<div class="cell_output docutils container">
<img alt="../_images/gnn_10_0.png" src="../_images/gnn_10_0.png" />
</div>
<p class="caption"><span class="caption-number">Fig. 3.3 </span><span class="caption-text">Intermediate step of the graph convolution layer. The center node is being updated by averaging its neighbors features.</span><a class="headerlink" href="#dframe" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="gcnanim">
<img alt="../_images/gcn.gif" src="../_images/gcn.gif" />
<p class="caption"><span class="caption-number">Fig. 3.4 </span><span class="caption-text">Animation of the graph convolution layer. The left is input, right is output node features. Note that two layers are shown (see title change).</span><a class="headerlink" href="#gcnanim" title="Permalink to this image">¶</a></p>
</div>
<p>To help understand the GCN layer, look at <a class="reference internal" href="#dframe"><span class="std std-numref">Fig. 3.3</span></a>. It shows an intermediate step of the GCN layer. Each node feature is represented here as a one-hot encoded vector at input. The animation in <a class="reference internal" href="#gcnanim"><span class="std std-numref">Fig. 3.4</span></a> shows the averaging process over neighbor features.  To make this animation easy to follow, the trainable weights and activation functions are not considered. Note that the animation repeats for a second layer. Watch how the “information” about there being an oxygen atom in the molecule is propogated only after two layers to each atom. All GNNs operate with similair approaches, so try to understand how this animation works.</p>
<div class="section" id="gcn-implementation">
<h3><span class="section-number">3.4.1. </span>GCN Implementation<a class="headerlink" href="#gcn-implementation" title="Permalink to this headline">¶</a></h3>
<p>Let’s now create a tensor implementation of the GCN. We’ll skip the activation and trainable weights for now.
We must first compute our rank 2 adjacency matrix. The <code class="docutils literal notranslate"><span class="pre">smiles2graph</span></code> code above computes an adjacency tensor with feature vectors. We can fix that with a simple reduction and add the identity at the same time</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">smiles2graph</span><span class="p">(</span><span class="s1">&#39;CO&#39;</span><span class="p">)</span>
<span class="n">adj_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">adj</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">adj</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">adj_mat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1., 1., 1., 1., 1., 0.],
       [1., 1., 0., 0., 0., 1.],
       [1., 0., 1., 0., 0., 0.],
       [1., 0., 0., 1., 0., 0.],
       [1., 0., 0., 0., 1., 0.],
       [0., 1., 0., 0., 0., 1.]])
</pre></div>
</div>
</div>
</div>
<p>To compute degree of each node, we can do another reduction:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">degree</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">adj_mat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">degree</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([5., 3., 2., 2., 2., 2.])
</pre></div>
</div>
</div>
</div>
<p>Now we can put all these pieces together into the Einstein equation</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># note to divide by degree, make the input 1 / degree</span>
<span class="n">new_nodes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i,ij,jk-&gt;ik&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">degree</span><span class="p">,</span> <span class="n">adj_mat</span><span class="p">,</span> <span class="n">nodes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">new_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1. 0. 0.]
[0.2 0.2 0.6]
</pre></div>
</div>
</div>
</div>
<p>To now implement this as a layer in Keras, we must put this code above into a new Layer subclass. The code is relatively straightforward, but you can read-up on the function names and Layer class in <a class="reference external" href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/">this tutorial</a>. The three main changes are that we create trainable parameters <code class="docutils literal notranslate"><span class="pre">self.w</span></code> and use them in the <code class="docutils literal notranslate"><span class="pre">einsum</span></code>, we use an activation <code class="docutils literal notranslate"><span class="pre">self.activation</span></code>, and we output both our new node features and the adjacency matrix. The reason to output the adjacency matrix is so that we can stack multiple GCN layers without having to pass the adjacency matrix each time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GCNLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Implementation of GCN as layer&#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># constructor, which just calls super constructor</span>
        <span class="c1"># and turns requested activation into a callable function</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GCNLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="c1"># create trainable weights</span>
        <span class="n">node_shape</span><span class="p">,</span> <span class="n">adj_shape</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">node_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">node_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># split input into nodes, adj</span>
        <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">inputs</span> 
        <span class="c1"># compute degree</span>
        <span class="n">degree</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">adj</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># GCN equation</span>
        <span class="n">new_nodes</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bi,bij,bjk,kl-&gt;bil&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">degree</span><span class="p">,</span> <span class="n">adj</span><span class="p">,</span> <span class="n">nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">new_nodes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">adj</span>
</pre></div>
</div>
</div>
</div>
<p>We can now try our our layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gcnlayer</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
<span class="c1"># we insert a batch axis here</span>
<span class="n">gcnlayer</span><span class="p">((</span><span class="n">nodes</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span><span class="o">...</span><span class="p">],</span> <span class="n">adj_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span><span class="o">...</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:Layer gcn_layer is casting an input tensor from dtype float64 to the layer&#39;s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(&#39;float64&#39;)`. To change just this layer, pass dtype=&#39;float64&#39; to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Tensor: shape=(1, 6, 3), dtype=float32, numpy=
 array([[[0.        , 0.        , 0.        ],
         [0.01062139, 0.07862942, 0.        ],
         [0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        ],
         [0.0488025 , 0.        , 0.        ]]], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=
 array([[[1., 1., 1., 1., 1., 0.],
         [1., 1., 0., 0., 0., 1.],
         [1., 0., 1., 0., 0., 0.],
         [1., 0., 0., 1., 0., 0.],
         [1., 0., 0., 0., 1., 0.],
         [0., 1., 0., 0., 0., 1.]]], dtype=float32)&gt;)
</pre></div>
</div>
</div>
</div>
<p>It outputs (1) the new node features and (2) the adjacency matrix. Let’s make sure we can stack these and apply the GCN multiple times</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span><span class="o">...</span><span class="p">],</span> <span class="n">adj_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span><span class="o">...</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">gcnlayer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>It works! Why do we see zeros though? Probably because we had negative numbers that were removed by our ReLU activation. This will be solved by training and increasing our dimension number.</p>
</div>
</div>
<div class="section" id="solubility-example">
<h2><span class="section-number">3.5. </span>Solubility Example<a class="headerlink" href="#solubility-example" title="Permalink to this headline">¶</a></h2>
<p>We’ll now revisit predicting solubility with GCNs. Remember before that we used the features included with the dataset. Now, however we can use the molecular structures directly. Our GCN layer outputs node-level features. To predict solubility, we need to get a graph-level feature. We’ll see later how to be more sophisticated in this process, but for now let’s just take the average over all node features after our GCN layers. This is simple, permutation invariant, and gets us from node-level to graph level. Here’s an implementation of this</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GRLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;A GNN layer that computes average over all node features&#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;GRLayer&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GRLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reduction</span>
    
</pre></div>
</div>
</div>
</div>
<p>To complete our deep solubility predictor, we can add some dense layers and make sure we have a single-output without activation since we’re doing regression. Note this model is defined using the <a class="reference external" href="https://keras.io/guides/functional_api/">Keras functional API</a> which is necessary when you have multiple inputs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ninput</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span><span class="mi">100</span><span class="p">,))</span>
<span class="n">ainput</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">,))</span>
<span class="c1"># GCN block</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)([</span><span class="n">ninput</span><span class="p">,</span> <span class="n">ainput</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GCNLayer</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># reduce to graph features</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">GRLayer</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># standard layers</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="s1">&#39;tanh&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="n">ninput</span><span class="p">,</span> <span class="n">ainput</span><span class="p">),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>where does the 100 come from? Well, this dataset has lots of elements so we cannout use our size 3 one-hot encondings because we’ll have more than 3 unique elements. We previously only had C, H and O. This is a good time to updaet our <code class="docutils literal notranslate"><span class="pre">smiles2graph</span></code> function to deal with this</p>
<div class="cell tag_hidden-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gen_smiles2graph</span><span class="p">(</span><span class="n">sml</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Argument for the RD2NX function should be a valid SMILES sequence</span>
<span class="sd">    returns: the graph</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">MolFromSmiles</span><span class="p">(</span><span class="n">sml</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">AddHs</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">order_string</span> <span class="o">=</span> <span class="p">{</span><span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">SINGLE</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">DOUBLE</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
                    <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">TRIPLE</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
                    <span class="n">rdkit</span><span class="o">.</span><span class="n">Chem</span><span class="o">.</span><span class="n">rdchem</span><span class="o">.</span><span class="n">BondType</span><span class="o">.</span><span class="n">AROMATIC</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">()))</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetAtoms</span><span class="p">():</span>
        <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">GetIdx</span><span class="p">(),</span> <span class="n">i</span><span class="o">.</span><span class="n">GetAtomicNum</span><span class="p">()]</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="n">adj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">GetBonds</span><span class="p">():</span>
        <span class="n">u</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span><span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>
        <span class="n">v</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">j</span><span class="o">.</span><span class="n">GetBeginAtomIdx</span><span class="p">(),</span><span class="n">j</span><span class="o">.</span><span class="n">GetEndAtomIdx</span><span class="p">())</span>        
        <span class="n">order</span> <span class="o">=</span> <span class="n">j</span><span class="o">.</span><span class="n">GetBondType</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_string</span><span class="p">:</span>
            <span class="n">order</span> <span class="o">=</span> <span class="n">order_string</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Warning</span><span class="p">(</span><span class="s1">&#39;Ignoring bond order&#39;</span> <span class="o">+</span> <span class="n">order</span><span class="p">)</span>
        <span class="n">adj</span><span class="p">[</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>        
        <span class="n">adj</span><span class="p">[</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">adj</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nodes</span><span class="p">,</span> <span class="n">adj</span> <span class="o">=</span> <span class="n">gen_smiles2graph</span><span class="p">(</span><span class="s1">&#39;CO&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="p">((</span><span class="n">nodes</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span><span class="o">...</span><span class="p">],</span> <span class="n">adj_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span><span class="o">...</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.04285163]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<p>It outputs one number! That’s always nice to have. Now we need to do some work to get a trainable dataset. Our dataset is a little bit complex because our features are tuples of tensors(<span class="math notranslate nohighlight">\(\mathbf{V}, \mathbf{E}\)</span>) so that our dataset is a tuple of tuples: <span class="math notranslate nohighlight">\(\left((\mathbf{V}, \mathbf{E}), y\right)\)</span>. We use a <strong>generator</strong>, which is just a python function that can return multiple times. Our function returns once for every training example. Then we have to pass it to the <code class="docutils literal notranslate"><span class="pre">from_generator</span></code> dataset constructor which requires explicit declaration of the shapes of these examples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">example</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">soldata</span><span class="p">)):</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">gen_smiles2graph</span><span class="p">(</span><span class="n">soldata</span><span class="o">.</span><span class="n">SMILES</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>        
        <span class="n">sol</span> <span class="o">=</span> <span class="n">soldata</span><span class="o">.</span><span class="n">Solubility</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">yield</span> <span class="n">graph</span><span class="p">,</span> <span class="n">sol</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_generator</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="n">output_types</span><span class="o">=</span><span class="p">((</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> 
                                      <span class="n">output_shapes</span><span class="o">=</span><span class="p">((</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">]),</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])),</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([])))</span>
</pre></div>
</div>
</div>
</div>
<p>Whew, that’s a lot. Now we can do our usual splitting of the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="mi">400</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And finally, time to train.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;validation&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gnn_38_0.png" src="../_images/gnn_38_0.png" />
</div>
</div>
<p>This model is definitely underfit. One reason is that our batch size is 1. This is an side-effect of making the number of atoms variable and then tensorflow has trouble batching together our data if there are two unknown dimensions. You can fix this by manually batching or padding all molecules to have as many atoms as the one with the max. In any case, this example shows how to use GCN layers in a complete model.</p>
</div>
<div class="section" id="message-passing-viewpoint">
<h2><span class="section-number">3.6. </span>Message Passing Viewpoint<a class="headerlink" href="#message-passing-viewpoint" title="Permalink to this headline">¶</a></h2>
<p>One way to more broadly view a GCN layer is that it is a kind of “message-passing” layer. You first compute a message coming from each neighboring node:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4d8ec7e9-0645-4c68-88d4-d8c5698e7e48">
<span class="eqno">(3.12)<a class="headerlink" href="#equation-4d8ec7e9-0645-4c68-88d4-d8c5698e7e48" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{e}_{{s_i}j} = \vec{n}_{{s_i}j} \mathbf{W}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(n_{{s_i}j}\)</span> means the <span class="math notranslate nohighlight">\(j\)</span>th neigbhor of node <span class="math notranslate nohighlight">\(i\)</span>. The <span class="math notranslate nohighlight">\(s_i\)</span> means senders to <span class="math notranslate nohighlight">\(i\)</span>. This is how a GCN computes the messages, it’s just a weight matrix times each neighbor node features. After getting the messages that will go to <span class="math notranslate nohighlight">\(\vec{e}_{{s_i}j}\)</span>, we aggregate them using a function which is permutation invariant to the order of neigbhors:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8a32bd4d-66bb-42b0-92a8-99b1e1ef55d9">
<span class="eqno">(3.13)<a class="headerlink" href="#equation-8a32bd4d-66bb-42b0-92a8-99b1e1ef55d9" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{e}_{i} = \frac{1}{|\vec{e}_{{s_i}j}|}\sum \vec{e}_{{s_i}j} 
\end{equation}\]</div>
<p>In the GCN this aggregation is just a mean. Finally, we update our node using the aggregated message in the GCN:</p>
<div class="amsmath math notranslate nohighlight" id="equation-200d4a98-eac5-4790-bbb6-a94118bec8d0">
<span class="eqno">(3.14)<a class="headerlink" href="#equation-200d4a98-eac5-4790-bbb6-a94118bec8d0" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{n}^{'}_{i} = \sigma(\vec{e}_i)
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(n^{'}\)</span> indicates the new node features. This is simply the activated aggregated message. Writing it out this way, you can see how it is possible to make small changes. One important paper by Gilmer et al. explored some of these choices and described how this general idea of message passing layers does well in learning to predict molecular energies from quantum mechanics <a class="bibtex reference internal" href="layers.html#gilmer2017neural" id="id2">[GSR+17]</a>. Examples of changes to the above GCN equations are to include edge information when computing the neighbor messages or use a dense neural network layer in place of <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
</div>
<div class="section" id="gated-graph-neural-network">
<h2><span class="section-number">3.7. </span>Gated Graph Neural Network<a class="headerlink" href="#gated-graph-neural-network" title="Permalink to this headline">¶</a></h2>
<p>One important variant of the message passing layer is the <strong>gated graph neural network</strong> (GGN) <a class="bibtex reference internal" href="layers.html#li2015gated" id="id3">[LTBZ15]</a>. It replaces the last equation, the node update, with</p>
<div class="amsmath math notranslate nohighlight" id="equation-17cf694b-99ca-4e0f-9300-79fcb8383afd">
<span class="eqno">(3.15)<a class="headerlink" href="#equation-17cf694b-99ca-4e0f-9300-79fcb8383afd" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{n}^{'}_{i} = \textrm{GRU}(\vec{n}_i, \vec{e}_i)
\end{equation}\]</div>
<p>where the <span class="math notranslate nohighlight">\(\textrm{GRU}(\cdot, \cdot)\)</span> is a gated recurrent unit<a class="bibtex reference internal" href="layers.html#chung2014empirical" id="id4">[CGCB14]</a>. The interesting property of a GRU is that it has trainable parameters, giving the model a bit more flexibility, but the GRU parameters do not change as you stack more layers. A GRU is usually used for modeling sequences of undetermined length, like a sentence. What’s nice about this is that you can stack infinite GGN layers without increasing the number of trainable parameters (assuming you make <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> the same at each layer). Thus GGNs are suited for large graphs, like a large protein or large unit cell.</p>
</div>
<div class="section" id="battaglia-general-equations">
<h2><span class="section-number">3.8. </span>Battaglia General Equations<a class="headerlink" href="#battaglia-general-equations" title="Permalink to this headline">¶</a></h2>
<p>As you can see, message passing layers is a general way to view GNN layers. Battaglia et al <a class="bibtex reference internal" href="layers.html#battaglia2018relational" id="id5">[BHB+18]</a> went further and created a general set of equations which captures nearly all GNNs. They broke the GNN layer equations down into 3 update equations, like the node update equation we saw in the message passing layer equations, and 3 aggregation equations. There is a new concept in these equations: graph feature vectors. A graph feature vector is a set of features which represent the whole graph or molecule. For example, when computing solubility it may have been useful to build-up a per-molecule feature vector that is eventually used to compute solubility. Any knid of per-molecule quantity like energy can be expressed as a graph-level feature vector.</p>
<p>The first step in these equations is updating the edge feature vectors, written as <span class="math notranslate nohighlight">\(\vec{e}_k\)</span>, which we haven’t seen yet but is certainly possible:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0c426183-c3b2-4fc5-a910-db5a8f5fcad2">
<span class="eqno">(3.16)<a class="headerlink" href="#equation-0c426183-c3b2-4fc5-a910-db5a8f5fcad2" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{e}^{'}_k = \phi^e\left( \vec{e}_k, \vec{v}_{rk}, \vec{v}_{sk}, \vec{u}\right)
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\vec{e}_k\)</span> is the feature vector of edge <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\vec{v}_{rk}\)</span> is the receiving node feature vector for edge <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\vec{v}_{sk}\)</span> is the sending node feature vector for edge <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(\vec{u}\)</span> is the global graph feature vector, and <span class="math notranslate nohighlight">\(\phi^e\)</span> is one of the three update functions that the define the GNN layer. Note that these are meant to be general expressions and you define <span class="math notranslate nohighlight">\(\phi^e\)</span> for your specific GNN layer. The output edge updates are then aggregated with the first aggregation function:</p>
<div class="amsmath math notranslate nohighlight" id="equation-30fd896d-567b-455e-b7a4-5d66ad446f94">
<span class="eqno">(3.17)<a class="headerlink" href="#equation-30fd896d-567b-455e-b7a4-5d66ad446f94" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\bar{e}^{'}_i = \rho^{e\rightarrow v}\left( E_i^{'}\right)
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho^{e\rightarrow v}\)</span> is our defined function and <span class="math notranslate nohighlight">\(E_i^{'}\)</span> represents all edges in or out of node i. Having our aggregated edge, which is equivalent to our <em>message</em> previously, we can compute the node update:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7c117e02-7a01-4e2d-8b5b-45bb45ff16ad">
<span class="eqno">(3.18)<a class="headerlink" href="#equation-7c117e02-7a01-4e2d-8b5b-45bb45ff16ad" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{v}^{'}_i = \phi^v\left( \bar{e}^{'}_i, \vec{v}_i, \vec{u}\right)
\end{equation}\]</div>
<p>This concludes the usual steps of a GNN layer. If you are updating global attributes or aggregating nodes or edges, the following additional steps may be defined:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9301eb2a-731a-47ab-ab41-9c07ff5bd4c7">
<span class="eqno">(3.19)<a class="headerlink" href="#equation-9301eb2a-731a-47ab-ab41-9c07ff5bd4c7" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\bar{e}^{'} = \rho^{e\rightarrow u}\left( E^{'}\right)
\end{equation}\]</div>
<p>This equation aggregates all messages across the whole graph. Then we can aggregate the new nodes across the whole graph:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5051395e-2748-4473-992d-835194bd7ef8">
<span class="eqno">(3.20)<a class="headerlink" href="#equation-5051395e-2748-4473-992d-835194bd7ef8" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\bar{v}^{'} = \rho^{v\rightarrow u}\left( V^{'}\right)
\end{equation}\]</div>
<p>Then, we can compute the update to the global feature vector as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-063f445e-02ad-4882-b663-c559989c8af7">
<span class="eqno">(3.21)<a class="headerlink" href="#equation-063f445e-02ad-4882-b663-c559989c8af7" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{u}^{'} = \phi^u\left( \bar{e}^{'},\bar{v}^{'}, \vec{u}\right)
\end{equation}\]</div>
<div class="section" id="reformulating-gcn-into-battaglia-equations">
<h3><span class="section-number">3.8.1. </span>Reformulating GCN into Battaglia equations<a class="headerlink" href="#reformulating-gcn-into-battaglia-equations" title="Permalink to this headline">¶</a></h3>
<p>Let’s see how the GCN is presented in this form. We first compute our neighbor messages for all possible neighbors. Since our graph is undirected, we’ll just by convention use the senders as the “neighbor”</p>
<div class="amsmath math notranslate nohighlight" id="equation-baa620b1-a452-46c1-94f4-300836691ed9">
<span class="eqno">(3.22)<a class="headerlink" href="#equation-baa620b1-a452-46c1-94f4-300836691ed9" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{e}^{'}_k = \phi^e\left( \vec{e}_k, \vec{v}_{rk}, \vec{v}_{sk}, \vec{u}\right) = \vec{v}_{sk} \mathbf{W}
\end{equation}\]</div>
<p>To aggregate our messages, we average them.</p>
<div class="amsmath math notranslate nohighlight" id="equation-52a8f6ae-9128-4821-a5b8-1f22097b4346">
<span class="eqno">(3.23)<a class="headerlink" href="#equation-52a8f6ae-9128-4821-a5b8-1f22097b4346" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\bar{e}^{'}_i = \rho^{e\rightarrow v}\left( E_i^{'}\right) = \frac{1}{|E_i^{'}|}\sum E_i^{'}
\end{equation}\]</div>
<p>Our node update is then the activation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4345250d-13c0-493a-93b7-d0cdddc0b0c6">
<span class="eqno">(3.24)<a class="headerlink" href="#equation-4345250d-13c0-493a-93b7-d0cdddc0b0c6" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\vec{v}^{'}_i = \phi^v\left( \bar{e}^{'}_i, \vec{v}_i, \vec{u}\right) = \sigma(\bar{e}^{'}_i)
\end{equation}\]</div>
<p>we could include the self-loop above using <span class="math notranslate nohighlight">\(\sigma(\bar{e}^{'}_i + \vec{v}_i)\)</span>. The other functions are not used so that is the complete set.</p>
</div>
</div>
<div class="section" id="cited-references">
<h2><span class="section-number">3.9. </span>Cited References<a class="headerlink" href="#cited-references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-dl/gnn-0"><dl class="citation">
<dt class="bibtex label" id="baluja1997329"><span class="brackets">BP97</span></dt>
<dd><p>Shumeet Baluja and Dean A. Pomerleau. Expectation-based selective attention for visual monitoring and control of a robot vehicle. <em>Robotics and Autonomous Systems</em>, 22(3):329 – 344, 1997. Robot Learning: The New Wave. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0921889097000468">http://www.sciencedirect.com/science/article/pii/S0921889097000468</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/S0921-8890(97)00046-8">doi:https://doi.org/10.1016/S0921-8890(97)00046-8</a>.</p>
</dd>
<dt class="bibtex label" id="battaglia2018relational"><span class="brackets"><a class="fn-backref" href="#id5">BHB+18</a></span></dt>
<dd><p>Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, and others. Relational inductive biases, deep learning, and graph networks. <em>arXiv preprint arXiv:1806.01261</em>, 2018.</p>
</dd>
<dt class="bibtex label" id="chung2014empirical"><span class="brackets"><a class="fn-backref" href="#id4">CGCB14</a></span></dt>
<dd><p>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. <em>arXiv preprint arXiv:1412.3555</em>, 2014.</p>
</dd>
<dt class="bibtex label" id="gilmer2017neural"><span class="brackets"><a class="fn-backref" href="#id2">GSR+17</a></span></dt>
<dd><p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. <em>arXiv preprint arXiv:1704.01212</em>, 2017.</p>
</dd>
<dt class="bibtex label" id="kipf2016semi"><span class="brackets"><a class="fn-backref" href="#id1">KW16</a></span></dt>
<dd><p>Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. <em>arXiv preprint arXiv:1609.02907</em>, 2016.</p>
</dd>
<dt class="bibtex label" id="li2015gated"><span class="brackets"><a class="fn-backref" href="#id3">LTBZ15</a></span></dt>
<dd><p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. <em>arXiv preprint arXiv:1511.05493</em>, 2015.</p>
</dd>
<dt class="bibtex label" id="luong2015effective"><span class="brackets">LPM15</span></dt>
<dd><p>Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. <em>arXiv preprint arXiv:1508.04025</em>, 2015.</p>
</dd>
<dt class="bibtex label" id="treisman1980feature"><span class="brackets">TG80</span></dt>
<dd><p>Anne M Treisman and Garry Gelade. A feature-integration theory of attention. <em>Cognitive psychology</em>, 12(1):97–136, 1980.</p>
</dd>
<dt class="bibtex label" id="vaswani2017attention"><span class="brackets">VSP+17</span></dt>
<dd><p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In <em>Advances in neural information processing systems</em>, 5998–6008. 2017.</p>
</dd>
<dt class="bibtex label" id="zhang2018gaan"><span class="brackets">ZSX+18</span></dt>
<dd><p>Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: gated attention networks for learning on large and spatiotemporal graphs. <em>arXiv preprint arXiv:1803.07294</em>, 2018.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="layers.html" title="previous page"><span class="section-number">2. </span>Standard Layers</a>
    <a class='right-next' id="next-link" href="attention.html" title="next page"><span class="section-number">4. </span>Attention Layers</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Andrew D. White<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <a href="http://thewhitelab.org">thewhitelab.org</a> <div id="wh-modal"> <button class="wh-venti-button" aria-label="close modal" id="wh-modal-close">✕</button> <img id="wh-modal-img"> </div>
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>